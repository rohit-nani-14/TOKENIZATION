#Tokenization code

!pip install nltk
import nltk
nltk.download('punkt')
text = "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action."
from nltk.tokenize import sent_tokenize
sent_tokenize_list = sent_tokenize(text)
print("\nSentence tokenizer:")
print(sent_tokenize_list)
from nltk.tokenize import word_tokenize
print("\nWord tokenizer:")
print(word_tokenize(text))
from nltk.tokenize import WordPunctTokenizer
punkt_word_tokenizer = WordPunctTokenizer()
print("\nPunkt word tokenizer:")
print(punkt_word_tokenizer.tokenize(text))
from nltk.tokenize import WordPunctTokenizer
word_punct_tokenizer = WordPunctTokenizer()
print("\nWord punct tokenizer:")
print(word_punct_tokenizer.tokenize(text))
